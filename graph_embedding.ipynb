{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff4831f-0904-4f47-93dc-aeca5a08761e",
   "metadata": {},
   "source": [
    "# グラフを対象とした畳み込み\n",
    "\n",
    "## 背景\n",
    "画像認識、音声認識、自然言語処理など、深層学習によって目覚ましい成果をあげている分野においては、画素が格子状に配置された2次元のデータや、単語が時間順に列として配置された1次元のデータをしゅな対象としている。\n",
    "\n",
    "一方、実世界におけるデータの多くはそのような規則的な構造ではなく、グラフ（ネットワーク）の形で表現されている。例えばSNSにおけるユーザーやフォロワー、Webにおけるページやハイパーリンク、タンパク質の相互作用ネットワーク、概念感の関係を表すナレッジグラフなどが挙げられる。\n",
    "\n",
    "従来の深層学習において、畳み込みニューなるネットワークは比較的に単純なグリッドや列を対象としていた。例えば画像認識においては、画素が格子状に並んだ２じげんのグリッドであり、自然言語処理においては１次元の単語列である。そのような規則的な構造においては、畳み込みのための近傍のフィルタを定義したりすることは比較的容易である。グラフにおいても、近傍のノードの特徴を用いて自身のノードの特徴を学習することによって、ノード分類やグラフ分類、リンク予測などのタスクを高精度に行うことが期待できる。しかしながら、ぐらづを対象とした畳み込みは、以下のような理由で単純ではなく。\n",
    "\n",
    "- 隣接頂点数が可変\n",
    "- 複雑なトポロジー\n",
    "- ノードが順序づけられていない"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b3193f-cb6d-4069-8b76-7d927a0f05fb",
   "metadata": {},
   "source": [
    "##  グラフを対象とした機械学習タスク\n",
    "\n",
    "グラフニューラルネットワーク（Graph Neural Networks, GNN）HA、グラフにおける構造情報も加味して各ノード及グラフ全体の特徴（表現）を学習するニューラルネットワークである。得られた各ノード及びグラフ全体の特徴は、ノード分類、グラフ分類、リンク予測、グラフ生成のモデル化などのタスクに用いられる。\n",
    "\n",
    "グラフ上では近接しているノード同士の特徴は類似していることが望ましい。その一方で、グラフにおいては極端に次数の多い（リンセルノードが多い）ノードも存在するため、近接しているノード同士の特徴を類似させ過ぎるとノード間の区別がつきにくくなる。また、畳み込みにおいて居所性が維持できない場合、非常に多くのノード情報を用いて必要が生じ、大規模グラフにおいては計算量が爆発してしまうという問題もある。\n",
    "\n",
    "以下の図を用いてグラフニューラルネットワークの概要について述べる。グラフのノードの特徴（表現）を学習によって近傍ノードの特徴を反映したものになっている。この学習後の特徴を利用して、ノード分類やグラフ分類を行う。\n",
    "\n",
    "<img src=\"./fig/1-1.png\" width=\"500\" title=\"GNNの原理\">\n",
    "\n",
    "ベクトルで表される日構造データを対象とした機械学習におけるタスクとしては、分類、回帰、クラスタリング、次元縮約などがある。一方、グラフニューラルネットワークではベクトルではなく、グラフを対象としており、そのタスクとしては以下のものがある。\n",
    "\n",
    "- ノードを対象としてタスク「**ノード分類**」\n",
    "- グラフを対象としたタスク「**グラフ分類**」\n",
    "- エージを対象としたタスク「**リンク予測**」\n",
    "- 生成モデルについてのタスク「**グラフ生成**」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96abafa-c8ab-44cd-95d4-f9f29831df81",
   "metadata": {},
   "source": [
    "## Graph neural networks:A review of methods and applications \n",
    "\n",
    "Zhou, Jie, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. \"Graph neural networks: A review of methods and applications.\" AI open 1 (2020): 57-81.\n",
    "\n",
    "Graphs are a kind of data structure which models a set of objects (nodes) and their relationships (edges). Recently, researches on analyzing graphs with machine learning have been receiving more and more attention because of thegreat expressivepower of graphs.\n",
    "\n",
    "1. **Recursive Neural Networks** are first utilized on directed acyclic graphs. Although being successful, the universal idea behind these methods is building state transition systems on graphs and iterate until convergence, which constrained the extendability and representation ability.\n",
    "2. Recent advancement of deep neural networks, especially **convolutional neural networks (CNNs)** (LeCun et al., 1998) result in the rediscovery of GNNs. The keys of CNNs are local connection, shared weights and the use of multiple layers. However, it is\n",
    "hard to define localized convolutional filters and pooling operators, which hinders the transformation of CNN from Euclidean domain to non-Euclidean domain.\n",
    "3. Extending deep neural models to non-Euclidean domains, which is generally referred to as **geometric deep learning**, has been an emerging research area.\n",
    "4. The other motivation comes from **graph representation learning**, which learns to represent graph nodes, edges or subgraphs by low-dimensional vectors. In the field of graph analysis, traditional machine learning approaches usually rely on hand engineered features and are limited by its inflexibility and high cost. Following the idea of representation learning and the success of word embedding, DeepWalk, regarded as the first graph embedding method based on representation learning, applies SkipGram model (Mikolov et al., 2013) on the generated random walks. Similar approaches such as node2vec (Grover and Leskovec, 2016), LINE (Tang et al., 2015) and TADW (Yang et al., 2015) also achieved breakthroughs.\n",
    "\n",
    "    However, these methods suffer from two severe drawbacks (Hamilton et al., 2017b). \n",
    "    1) First, no parameters are shared between nodes in the encoder, which leads to computationally inefficiency, since it means the number of parameters grows linearly with the number of nodes.\n",
    "    2) Second, the direct embedding methods lack the ability of generalization, which means they cannot deal with dynamic graphs or generalize to new graphs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
