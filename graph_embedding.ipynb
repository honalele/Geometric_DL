{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff4831f-0904-4f47-93dc-aeca5a08761e",
   "metadata": {},
   "source": [
    "# グラフを対象とした畳み込み\n",
    "\n",
    "## 背景\n",
    "画像認識、音声認識、自然言語処理など、深層学習によって目覚ましい成果をあげている分野においては、画素が格子状に配置された2次元のデータや、単語が時間順に列として配置された1次元のデータをしゅな対象としている。\n",
    "\n",
    "一方、実世界におけるデータの多くはそのような規則的な構造ではなく、グラフ（ネットワーク）の形で表現されている。例えばSNSにおけるユーザーやフォロワー、Webにおけるページやハイパーリンク、タンパク質の相互作用ネットワーク、概念感の関係を表すナレッジグラフなどが挙げられる。\n",
    "\n",
    "従来の深層学習において、畳み込みニューなるネットワークは比較的に単純なグリッドや列を対象としていた。例えば画像認識においては、画素が格子状に並んだ２じげんのグリッドであり、自然言語処理においては１次元の単語列である。そのような規則的な構造においては、畳み込みのための近傍のフィルタを定義したりすることは比較的容易である。グラフにおいても、近傍のノードの特徴を用いて自身のノードの特徴を学習することによって、ノード分類やグラフ分類、リンク予測などのタスクを高精度に行うことが期待できる。しかしながら、ぐらづを対象とした畳み込みは、以下のような理由で単純ではなく。\n",
    "\n",
    "- 隣接頂点数が可変\n",
    "- 複雑なトポロジー\n",
    "- ノードが順序づけられていない"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b3193f-cb6d-4069-8b76-7d927a0f05fb",
   "metadata": {},
   "source": [
    "##  グラフを対象とした機械学習タスク\n",
    "\n",
    "グラフニューラルネットワーク（Graph Neural Networks, GNN）HA、グラフにおける構造情報も加味して各ノード及グラフ全体の特徴（表現）を学習するニューラルネットワークである。得られた各ノード及びグラフ全体の特徴は、ノード分類、グラフ分類、リンク予測、グラフ生成のモデル化などのタスクに用いられる。\n",
    "\n",
    "グラフ上では近接しているノード同士の特徴は類似していることが望ましい。その一方で、グラフにおいては極端に次数の多い（リンセルノードが多い）ノードも存在するため、近接しているノード同士の特徴を類似させ過ぎるとノード間の区別がつきにくくなる。また、畳み込みにおいて居所性が維持できない場合、非常に多くのノード情報を用いて必要が生じ、大規模グラフにおいては計算量が爆発してしまうという問題もある。\n",
    "\n",
    "以下の図を用いてグラフニューラルネットワークの概要について述べる。グラフのノードの特徴（表現）を学習によって近傍ノードの特徴を反映したものになっている。この学習後の特徴を利用して、ノード分類やグラフ分類を行う。\n",
    "\n",
    "<img src=\"./fig/1-1.png\" width=\"500\" title=\"GNNの原理\">\n",
    "\n",
    "ベクトルで表される日構造データを対象とした機械学習におけるタスクとしては、分類、回帰、クラスタリング、次元縮約などがある。一方、グラフニューラルネットワークではベクトルではなく、グラフを対象としており、そのタスクとしては以下のものがある。\n",
    "\n",
    "- ノードを対象としてタスク「**ノード分類**」\n",
    "- グラフを対象としたタスク「**グラフ分類**」\n",
    "- エージを対象としたタスク「**リンク予測**」\n",
    "- 生成モデルについてのタスク「**グラフ生成**」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96abafa-c8ab-44cd-95d4-f9f29831df81",
   "metadata": {},
   "source": [
    "## Graph neural networks:A review of methods and applications \n",
    "\n",
    "Zhou, Jie, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. \"Graph neural networks: A review of methods and applications.\" AI open 1 (2020): 57-81.\n",
    "\n",
    "\n",
    "### History of GNNs\n",
    "Graphs are a kind of data structure which models a set of objects (nodes) and their relationships (edges). Recently, researches on analyzing graphs with machine learning have been receiving more and more attention because of thegreat expressivepower of graphs.\n",
    "\n",
    "1. **Recursive Neural Networks** are first utilized on directed acyclic graphs. Although being successful, the universal idea behind these methods is building state transition systems on graphs and iterate until convergence, which constrained the extendability and representation ability.\n",
    "2. Recent advancement of deep neural networks, especially **convolutional neural networks (CNNs)** (LeCun et al., 1998) result in the rediscovery of GNNs. The keys of CNNs are local connection, shared weights and the use of multiple layers. However, it is\n",
    "hard to define localized convolutional filters and pooling operators, which hinders the transformation of CNN from Euclidean domain to non-Euclidean domain.\n",
    "3. Extending deep neural models to non-Euclidean domains, which is generally referred to as **geometric deep learning**, has been an emerging research area.\n",
    "4. The other motivation comes from **graph representation learning**, which learns to represent graph nodes, edges or subgraphs by low-dimensional vectors. In the field of graph analysis, traditional machine learning approaches usually rely on hand engineered features and are limited by its inflexibility and high cost. Following the idea of representation learning and the success of word embedding, DeepWalk, regarded as the first graph embedding method based on representation learning, applies SkipGram model (Mikolov et al., 2013) on the generated random walks. Similar approaches such as node2vec (Grover and Leskovec, 2016), LINE (Tang et al., 2015) and TADW (Yang et al., 2015) also achieved breakthroughs.\n",
    "\n",
    "    However, these methods suffer from two severe drawbacks (Hamilton et al., 2017b). \n",
    "    1) First, no parameters are shared between nodes in the encoder, which leads to computationally inefficiency, since it means the number of parameters grows linearly with the number of nodes.\n",
    "    2) Second, the direct embedding methods lack the ability of generalization, which means they cannot deal with dynamic graphs or generalize to new graphs.\n",
    "  \n",
    "### Approaches   \n",
    "There have also been several surveys focusing on some specific graph learning fields.\n",
    "\n",
    " - Sun et al. (2018) and Chen et al. (2020a) give detailed overviews for **adversarial learning** methods on graphs, including graph data attack and defense.\n",
    " - Lee et al. (2018a) provide a review over **graph attention models**.\n",
    " - The paper proposed by Yang et al. (2020) focuses on **heterogeneous graph representation learning**, where nodes or edges are of multiple types.\n",
    " - Huang et al. (2020) review over **existing GNN models for dynamic graphs**.\n",
    " - Peng et al. (2020) summarize **graph embeddings methods for combinatorial optimization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea85f4",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "In later sections, we denote a graph as $G=(V, E)$, where $|V|=N$ is the number of nodes in the graph and $|E|=N^e$ is the number of edges. $\\mathbf{A} \\in$ $\\mathbb{R}^{N \\times N}$ is the adjacency matrix. For graph representation learning, we use $\\mathbf{h}_v$ and $\\boldsymbol{o}_v$ as the hidden state and output vector of node $v$. The detailed descriptions of the notations could be found in Table 1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9c946",
   "metadata": {},
   "source": [
    "## Find graph structure\n",
    "\n",
    "At first, we have to find out the graph structure in the application. There are usually two scenarios: structural scenarios and non-structural\n",
    "scenarios. \n",
    " 1. In structural scenarios, the graph structure is explicit in the applications, such as applications on molecules, physical systems, knowledge graphs and so on. \n",
    " 2. In non-structural scenarios, graphs are implicit so that we have to first build the graph from the task, such as building a fully-connected “word” graph for text or building a scene graph for an image. After we get the graph, the later design process attempts to find an optimal GNN model on this specific graph.\n",
    "\n",
    "After we get the graph in the application, we then have to find out the graph type and its scale. Graphs with complex types could provide more information on nodes and their connections. Graphs are usually categorized as:\n",
    " - **Directed/Undirected Graphs**. Edges in directed graphs are all directed from one node to another, which provide more information than undirected graphs. Each edge in undirected graphs can also be regarded as two directed edges.\n",
    " - **Homogeneous/Heterogeneous Graphs**. Nodes and edges in homogeneous graphs have same types, while nodes and edges have different types in heterogeneous graphs. Types for nodes and edge play important roles in heterogeneous graphs and should be further considered.\n",
    " - **Static/Dynamic Graphs**. When input features or the topology of the graph vary with time, the graph is regarded as a dynamic graph. The time information should be carefully considered in dynamic graphs.\n",
    "\n",
    "Note these categories are **orthogonal**, which means these types can be combined, e.g. one can deal with a dynamic directed heterogeneous\n",
    "graph. There are also several other graph types designed for different tasks such as hypergraphs and signed graphs. We will not enumerate all types here but the most important idea is to consider the additional information provided by these graphs. Once we specify the graph type, the additional information provided by these graph types should be further considered in the design process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81d1f6",
   "metadata": {},
   "source": [
    "## Design loss function\n",
    "\n",
    "For graph learning tasks, there are usually three kinds of tasks:\n",
    " 1. _Node-level_ tasks focus on nodes, which include node classification, node regression, node clustering, etc. Node classification tries to categorize nodes into several classes, and node regression predicts a continuous value for each node. Node clustering aims to partition the nodes into several disjoint groups, where similar nodes should be in the same group.\n",
    " 2. _Edge-level_ tasks are edge classification and link prediction, which require the model to classify edge types or predict whether there is an\n",
    "edge existing between two given nodes.\n",
    " 3. _Graph-level_ tasks include graph classification, graph regression, and graph matching, all of which need the model to learn graph representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf7b6c",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "## Build model using computational modules\n",
    "\n",
    "Finally, we can start building the model using the computational modules. Some commonly used computational modules are:\n",
    " 1. **Propagation Module**. The propagation module is used to propagate information between nodes so that the aggregated information could capture both feature and topological information. In propagation modules, the convolution operator and recurrent operator are usually used to aggregate information from neighbors while the skip connection operation is used to gather information from historical representations of nodes and mitigate the over-smoothing problem.\n",
    " 2. **Sampling Module**. When graphs are large, sampling modules are usually needed to conduct propagation on graphs. The sampling\n",
    "module is usually combined with the propagation module.\n",
    " 3. **Pooling Module**. When we need the representations of high-level subgraphs or graphs, pooling modules are needed to extract information from nodes.\n",
    "\n",
    "\n",
    " Following is the figure of how to build a GNN.\n",
    " <img src=\"./fig/1-2.png\" width=\"800\" title=\"GNNの原理\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633770e3",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
